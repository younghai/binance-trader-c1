{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from common_utils import make_dirs\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"rawdata_dir\": \"../../../storage/dataset/rawdata/csv/\",\n",
    "    \"data_store_dir\": \"../../../storage/dataset/dataset_10m/\",\n",
    "    \"winsorize_threshold\": 0.6,\n",
    "    \"lookahead_window\": 10,\n",
    "    \"train_ratio\": 0.7,\n",
    "    \"q_threshold\": 8,\n",
    "}\n",
    "COLUMNS = [\"open\", \"high\", \"low\", \"close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rawdata(file_name):\n",
    "    rawdata = pd.read_csv(file_name, header=0, index_col=0)[COLUMNS]\n",
    "    rawdata.index = pd.to_datetime(rawdata.index)\n",
    "\n",
    "    return rawdata\n",
    "\n",
    "\n",
    "def _build_feature_by_rawdata(rawdata):\n",
    "    returns = (\n",
    "        rawdata.pct_change(1, fill_method=None)\n",
    "        .iloc[1:]\n",
    "        .rename(columns={key: key + \"_return\" for key in COLUMNS})\n",
    "    )\n",
    "\n",
    "    inner_changes = []\n",
    "    for column_pair in sorted(list(combinations(COLUMNS, 2))):\n",
    "        inner_changes.append(\n",
    "            rawdata[list(column_pair)]\n",
    "            .pct_change(1, axis=1, fill_method=None)[column_pair[-1]]\n",
    "            .rename(\"_\".join(column_pair) + \"_change\")\n",
    "        )\n",
    "\n",
    "    inner_changes = pd.concat(inner_changes, axis=1).reindex(returns.index)\n",
    "\n",
    "    return pd.concat([returns, inner_changes], axis=1).sort_index()\n",
    "\n",
    "\n",
    "def _build_fwd_returns_by_rawdata(\n",
    "    rawdata, lookahead_window, column_pairs=[(\"close\", \"high\"), (\"close\", \"low\")]\n",
    "):\n",
    "    fwd_returns = []\n",
    "    for column_pair in column_pairs:\n",
    "        partial_fwd_returns = []\n",
    "        for window in range(1, lookahead_window + 1):\n",
    "            colum_pair_df = rawdata[list(column_pair)].copy().sort_index()\n",
    "            colum_pair_df.columns = [0, 1]\n",
    "\n",
    "            colum_pair_df[1] = colum_pair_df[1].shift(-window)\n",
    "            partial_fwd_return = colum_pair_df.pct_change(1, axis=1, fill_method=None)[\n",
    "                1\n",
    "            ].rename(f\"fwd_return({window})\")\n",
    "            partial_fwd_returns.append(partial_fwd_return)\n",
    "\n",
    "        partial_fwd_returns = pd.concat(partial_fwd_returns, axis=1).sort_index()\n",
    "        partial_fwd_returns.columns = [\n",
    "            \"_\".join(column_pair) + \"_\" + column\n",
    "            for column in partial_fwd_returns.columns\n",
    "        ]\n",
    "        fwd_returns.append(partial_fwd_returns)\n",
    "\n",
    "    return pd.concat(fwd_returns, axis=1).sort_index()\n",
    "\n",
    "\n",
    "def _build_bins(rawdata, lookahead_window):\n",
    "    column_pair = (\"close\", \"close\")\n",
    "\n",
    "    # build fwd_return(window)\n",
    "    colum_pair_df = rawdata[list(column_pair)].copy().sort_index()\n",
    "    colum_pair_df.columns = [0, 1]\n",
    "\n",
    "    colum_pair_df[1] = colum_pair_df[1].shift(-lookahead_window)\n",
    "    fwd_return = (\n",
    "        colum_pair_df.pct_change(1, axis=1, fill_method=None)[1]\n",
    "        .rename(f\"fwd_return({lookahead_window})\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    _, bins = pd.qcut(fwd_return.dropna(), 10, retbins=True, labels=False)\n",
    "    bins = np.concatenate([[-np.inf], bins[1:-1], [np.inf]])\n",
    "\n",
    "    return bins\n",
    "\n",
    "\n",
    "def _build_label_by_rawdata(rawdata, lookahead_window, q_threshold):\n",
    "    fwd_returns = _build_fwd_returns_by_rawdata(\n",
    "        rawdata=rawdata, lookahead_window=lookahead_window\n",
    "    )\n",
    "    bins = _build_bins(rawdata=rawdata, lookahead_window=lookahead_window)\n",
    "\n",
    "    quantile_df = fwd_returns.dropna().apply(\n",
    "        lambda x: x.parallel_apply(partial(compute_quantile, bins=bins))\n",
    "    )\n",
    "\n",
    "    total_positive_moving = (quantile_df >= q_threshold).any(axis=1)\n",
    "    total_negative_moving = (quantile_df <= 9 - q_threshold).any(axis=1)\n",
    "    static_moving = ~total_positive_moving & ~total_negative_moving\n",
    "\n",
    "    positive_negative_moving = total_positive_moving & total_negative_moving\n",
    "    positive_moving = total_positive_moving & ~positive_negative_moving\n",
    "    negative_moving = total_negative_moving & ~positive_negative_moving\n",
    "\n",
    "    # (0: Increases, 1: Decreases, 2: (Increases, Decreases), 3: Static)\n",
    "    label = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                positive_moving[positive_moving].astype(int) * 0,\n",
    "                negative_moving[negative_moving].astype(int) * 1,\n",
    "                positive_negative_moving[positive_negative_moving].astype(int) * 2,\n",
    "                static_moving[static_moving].astype(int) * 3,\n",
    "            ]\n",
    "        )\n",
    "        .rename(\"label\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    assert not any(label.index.duplicated())\n",
    "\n",
    "    return label\n",
    "\n",
    "\n",
    "def compute_quantile(x, bins):\n",
    "    for idx in range(len(bins) - 1):\n",
    "        if bins[idx] < x <= bins[idx + 1]:\n",
    "            return idx\n",
    "\n",
    "    raise RuntimeError(\"unreachable\")\n",
    "\n",
    "\n",
    "def build_features(file_names):\n",
    "    features = []\n",
    "    for file_name in tqdm(file_names):\n",
    "        coin_pair = file_name.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        rawdata = load_rawdata(file_name=file_name)\n",
    "        feature = _build_feature_by_rawdata(rawdata=rawdata)\n",
    "        feature.columns = pd.MultiIndex.from_tuples(\n",
    "            sorted([(coin_pair, column) for column in feature.columns])\n",
    "        )\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return pd.concat(features, axis=1).dropna().sort_index()\n",
    "\n",
    "\n",
    "def build_labels(file_names, lookahead_window, q_threshold):\n",
    "    labels = []\n",
    "    for file_name in tqdm(file_names):\n",
    "        coin_pair = file_name.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        rawdata = load_rawdata(file_name=file_name)\n",
    "        label = _build_label_by_rawdata(\n",
    "            rawdata=rawdata, lookahead_window=lookahead_window, q_threshold=q_threshold\n",
    "        ).rename(coin_pair)\n",
    "        labels.append(label)\n",
    "\n",
    "    return pd.concat(labels, axis=1).dropna().sort_index()\n",
    "\n",
    "\n",
    "def build_pricing(file_names):\n",
    "    pricing = []\n",
    "    for file_name in tqdm(file_names):\n",
    "        coin_pair = file_name.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        close = load_rawdata(file_name=file_name)[\"close\"].rename(coin_pair)\n",
    "        pricing.append(close)\n",
    "\n",
    "    return pd.concat(pricing, axis=1).dropna().sort_index()\n",
    "\n",
    "\n",
    "def build_scaler(features):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(features)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "\n",
    "def preprocess_features(features, scaler, winsorize_threshold):\n",
    "    index = features.index\n",
    "    columns = features.columns\n",
    "\n",
    "    processed_features = pd.DataFrame(\n",
    "        scaler.transform(features), index=index, columns=columns\n",
    "    )\n",
    "\n",
    "    # winsorize\n",
    "    return processed_features.clip(-winsorize_threshold, winsorize_threshold)\n",
    "\n",
    "\n",
    "def store_artifacts(features, labels, pricing, scaler, train_ratio, data_store_dir):\n",
    "    # Make dirs\n",
    "    train_data_store_dir = os.path.join(data_store_dir, \"train\")\n",
    "    test_data_store_dir = os.path.join(data_store_dir, \"test\")\n",
    "    make_dirs([train_data_store_dir, test_data_store_dir])\n",
    "\n",
    "    # Store\n",
    "    boundary_index = int(len(features.index) * train_ratio)\n",
    "    features.iloc[:boundary_index].to_csv(\n",
    "        os.path.join(train_data_store_dir, \"X.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "    features.iloc[boundary_index:].to_csv(\n",
    "        os.path.join(test_data_store_dir, \"X.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "\n",
    "    labels.iloc[:boundary_index].to_csv(\n",
    "        os.path.join(train_data_store_dir, \"Y.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "    labels.iloc[boundary_index:].to_csv(\n",
    "        os.path.join(test_data_store_dir, \"Y.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "\n",
    "    pricing.iloc[:boundary_index].to_csv(\n",
    "        os.path.join(train_data_store_dir, \"pricing.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "    pricing.iloc[boundary_index:].to_csv(\n",
    "        os.path.join(test_data_store_dir, \"pricing.csv\"), compression=\"gzip\"\n",
    "    )\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(data_store_dir, \"scaler.pkl\"))\n",
    "    \n",
    "    with open(os.path.join(data_store_dir, \"tradable_coins.txt\"), 'w') as f:\n",
    "        f.write('\\n'.join(pricing.columns.tolist()))\n",
    "\n",
    "\n",
    "def main(\n",
    "    rawdata_dir=CONFIG[\"rawdata_dir\"],\n",
    "    data_store_dir=CONFIG[\"data_store_dir\"],\n",
    "    winsorize_threshold=CONFIG[\"winsorize_threshold\"],\n",
    "    lookahead_window=CONFIG[\"lookahead_window\"],\n",
    "    q_threshold=CONFIG[\"q_threshold\"],\n",
    "    train_ratio=CONFIG[\"train_ratio\"],\n",
    "):\n",
    "    # Make dirs\n",
    "    make_dirs([data_store_dir])\n",
    "\n",
    "    # Set file_names\n",
    "    file_names = sorted(glob(os.path.join(rawdata_dir, \"*\")))\n",
    "\n",
    "    # Build features\n",
    "    features = build_features(file_names)\n",
    "    scaler = build_scaler(features)\n",
    "\n",
    "    features = preprocess_features(\n",
    "        features=features, scaler=scaler, winsorize_threshold=winsorize_threshold\n",
    "    )\n",
    "\n",
    "    # Build labels\n",
    "    labels = build_features(file_names)\n",
    "\n",
    "    # Build pricing\n",
    "    pricing = build_pricing(file_names)\n",
    "\n",
    "    # Masking with common index\n",
    "    common_index = features.index & labels.index\n",
    "    features = features.reindex(common_index).sort_index()\n",
    "    labels = labels.reindex(common_index).sort_index()\n",
    "    pricing = pricing.reindex(common_index).sort_index()\n",
    "\n",
    "    # Store Artifacts\n",
    "    store_artifacts(\n",
    "        features=features,\n",
    "        labels=labels,\n",
    "        pricing=pricing,\n",
    "        scaler=scaler,\n",
    "        train_ratio=train_ratio,\n",
    "        data_store_dir=data_store_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:49<00:00,  3.64s/it]\n",
      "100%|██████████| 30/30 [01:31<00:00,  3.05s/it]\n",
      "100%|██████████| 30/30 [01:41<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
